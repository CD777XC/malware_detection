# Importing common libraries
import pandas as pd

# Importing preprocessing libraries functions
from sklearn.preprocessing import RobustScaler, MinMaxScaler
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer
import joblib

# Loading environment variable
import os
from dotenv import load_dotenv
load_dotenv()

# Importing optimal features
from features_select import optimal_features

####################  Data Preprocessing  ####################

data = pd.read_csv(os.getenv("DATA"), sep="|")

# Defining X and y on a copy of data
X = data.drop(columns="legitimate").copy()
y = data['legitimate'].copy()

# Dropping numerical object columns
X = X.select_dtypes(exclude="object")

# Defining X features selected
X = X[optimal_features]

r_scaler_features = ['ImageBase', 'ResourcesMinSize', 'Subsystem'] # Selection done after boxplots visualization

####################  Preproc Pipeline  ####################

print(f'''{5*"#"} Building pipeline ⚙️ {5*"#"}''')

preproc_transformer = make_column_transformer(
    (RobustScaler(), r_scaler_features),
    (MinMaxScaler(), X.drop(columns=r_scaler_features).columns),
    remainder='passthrough'
)

preproc_pipe = make_pipeline(
    preproc_transformer
)
preproc_pipe.fit(X)

# Define the path to save the model
model_dir = 'models'
pipe_path = os.path.join(model_dir, 'preproc_pipe.pkl')

# Check if the directory exists, if not, create it
if not os.path.exists(model_dir):
    os.makedirs(model_dir)
    print(f"Directory {model_dir} created")

# Save the model using joblib
joblib.dump(preproc_pipe, pipe_path)
print(f"Pipeline saved to {pipe_path}")

print('✅')
