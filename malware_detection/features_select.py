# Importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import os
from dotenv import load_dotenv

from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance

####################  Functions  ########################

# Defining functions to be used
def significant_corr_pca(df, pos_threshold, neg_threshold):
    # Return a list of columns with high PCA correlation
    # Requires a PCA_df
    pos_corr = df.where(df > pos_threshold).where(df < 1).count()
    neg_corr = df.where(df < neg_threshold).where(df > -1).count()
    col_list = [column for column in pos_corr.keys() if pos_corr[column] > 0]
    col_list += [column for column in neg_corr.keys() if neg_corr[column] > 0]
    return col_list

def significant_corr(df, pos_threshold, neg_threshold):
    # Return a list of columns with high correlation
    pos_corr = df.corr().where(df.corr() > pos_threshold).where(df.corr() < 1).count()
    neg_corr = df.corr().where(df.corr() < neg_threshold).where(df.corr() > -1).count()
    col_list = [column for column in pos_corr.keys() if pos_corr[column] > 0]
    col_list += [column for column in neg_corr.keys() if neg_corr[column] > 0]
    return col_list

################  Data Preprocessing  ####################

# Loading .env variables
load_dotenv()

# Assigning dataset to dataframe
data = pd.read_csv(os.getenv("DATA"), sep="|")

# Defining X and y on a copy of data
X = data.drop(columns="legitimate").copy()
y = data['legitimate'].copy()

# Dropping numerical object columns
X = X.select_dtypes(exclude="object")
features_name = X.columns

# Scaling X for features selection
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler().set_output(transform='pandas') # Scales and return output as pandas df
X = scaler.fit_transform(X)

# Instanciating and fitting PCA model
pca = PCA(random_state=42)
pca.fit_transform(X)

# Creating a df with principal components
pca_corr = pca.components_
pca_corr = pd.DataFrame(pca_corr, columns=pca.feature_names_in_, index=X.columns)

# Extracting only correlations above 0.9 and below -0.65
high_pca_corr_col = significant_corr_pca(pca_corr, .9, -.65)

# Splitting dataset on train and test for Random Forest
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)

# Instanciating and fitting Random Forest
rf = RandomForestClassifier(random_state=0)
rf.fit(X_train, y_train)

# Feature importance based on mean
importances = rf.feature_importances_
std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)

# Creating df based on features importances
importances_df = pd.DataFrame(importances, index=features_name)
sorted_importances_df = importances_df.sort_values(by=0, ascending=False) # Sorting values

# Feature importance based on permutation
permutation_result = permutation_importance(
    rf,
    X_test, y_test,
    n_repeats=10,
    random_state=42,
    n_jobs=-1)

# Creating df based on permutation importances mean
permutation_importance_df = pd.DataFrame(permutation_result.importances_mean, index=features_name)
sorted_permutation_importance_df = permutation_importance_df.sort_values(by=0, ascending=False) # Sorting values

################  Selecting Features  ####################

pca = high_pca_corr_col
mean = sorted_importances_df.head(10).index,
permutation = sorted_permutation_importance_df.head().index
